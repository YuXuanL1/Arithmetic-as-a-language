# Arithmetic-as-a-language
treat arithmetic expressions as a language and use recurrent neural networks (RNN, LSTM) to train a sequence generation model for this special language.

## dataset
[Arithmetic dataset](https://drive.google.com/file/d/1cMuL3hF9jefka9RyF4gEBIGGeFGZYHE-/view?usp=sharing)
- Train split: 2,369,250 pieces
- Eval split: 263,250 pieces
- Each data piece: A 2~3-number equation, each number is in [0, 50),
  - e.g. (10 + 4) * 2 = and the answer is 28
  - The operations include: +, -, *, ()
![螢幕擷取畫面 2025-03-26 232431](https://github.com/user-attachments/assets/44f7e80b-f2d6-4390-829a-804b07c1eadc)

## data preprocessing
Process the data into the format required for model’s input and output.
The model is required to make predictions only for the tokens
following the '=' symbol in the input. Any output generated by the model
before the '=' symbol is irrelevant and should be excluded from the loss
calculation during training. Here we replace them to ‘<pad>’

## model
LSTM with teacher forcing

Teacher forcing is a training technique commonly used in sequence-based models. In teacher forcing, during training, instead of using the model’s predicted output as input for the next time step, the true target (the ground truth) from the training data is fed as the next input.
e.g. 1+2-3=0
the model input is: “1+2-3=”
instead of running several forward pass to generate the whole sequence, we input: “1+2+3=0” and predict p(‘0’|‘1+2+3=’) and p(‘<eos>’|‘1+2+3=0’)

![螢幕擷取畫面 2025-03-27 005316](https://github.com/user-attachments/assets/7fddb1b1-354d-4456-b388-f2a571b5af60)

## for more details
[report](https://github.com/user-attachments/files/19472111/NLP_HW2_NCCU_110306085.docx)
